{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044803a-3fe2-4297-ad71-c93ae2e078f5",
   "metadata": {},
   "source": [
    "# Part 2: Evaluating our LLM application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c46c-7650-484b-a792-7eaf62c0e82e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8d41d-39d8-4913-9a06-8e0b0df768c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216a6ba",
   "metadata": {},
   "source": [
    "### PLEASE PASTE YOUR OWN API KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119575d6-b0cc-49f3-a118-46bc3adf8189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9df539-d629-4e8b-9d93-2a2da5cd1b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916081f-df18-42bf-ac23-5047f6f5e52d",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cedb0a-a7b3-4194-88a9-355122cd8a00",
   "metadata": {},
   "source": [
    "### Golden Context Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31583b8a-bd08-4054-95f6-2ae5256e6a21",
   "metadata": {},
   "source": [
    "Given a set of queries, we would have the correct sources that answer those queries, and optionally the correct answer that should be returned by the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8495831b-c4f9-4e84-a211-bc37ecf369e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2966"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "golden_dataset_path = Path(\"../datasets/eval_qst.json\")\n",
    "data = []\n",
    "# Read the JSON file\n",
    "with open(golden_dataset_path, 'r', encoding='utf-8') as json_file:\n",
    "    # Load the JSON data into a list\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187c2d5-5c4d-4b9a-9e5b-5f37c9e92b36",
   "metadata": {},
   "source": [
    "Our dataset contains 'question', and 'source' pairs. If we have an **ideal** context dataset, it is the best option for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2300e0fc-3e65-4f43-98b6-3564bc1ccb3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the side effects of doxycycline?',\n",
       "  'source': 'https://www.drugs.com/doxycycline.html',\n",
       "  'text': '(hives, difficult breathing, swelling in your face or throat) or a severe skin reaction (fever, sore throat, burning in your eyes, skin pain, red or purple skin rash that spreads and causes blistering and peeling). Seek medical treatment if you have a serious drug reaction that can affect many parts of your body. Symptoms may include: skin rash, fever, swollen glands, flu-like symptoms, muscle aches, severe weakness, unusual bruising, or yellowing of your skin or eyes. This reaction may occur several weeks after you began using doxycycline. Doxycycline may cause serious side effects. Call your doctor at once if you have: severe stomach pain, diarrhea that is watery or bloody; throat irritation, trouble swallowing; chest pain, irregular heart rhythm, feeling short of breath; little or no urination; low white blood cell counts - fever, chills, swollen glands, body aches, weakness, pale skin, easy bruising or bleeding; severe headaches, ringing in your ears, dizziness, nausea, vision problems, pain behind your eyes; loss of appetite, upper stomach pain (that may spread to your back), tiredness, nausea or vomiting, fast heart rate, dark urine, jaundice (yellowing of the skin or eyes). Common side effects of doxycycline may include: nausea and vomiting; upset stomach; loss of appetite; mild diarrhea; skin rash or itching; darkened skin color; vaginal itching or discharge.'},\n",
       " {'question': 'What are the side effects of spironolactone?',\n",
       "  'source': 'https://www.drugs.com/spironolactone.html',\n",
       "  'text': 'hives ; difficulty breathing; swelling of your face, lips, tongue, or throat. Call your doctor at once if you have: a light-headed feeling, like you might pass out; little or no urination; high potassium level - nausea , weakness, tingly feeling, chest pain, irregular heartbeats, loss of movement; o signs of other electrolyte imbalances - increased thirst or urination, confusion, vomiting , muscle pain, slurred speech, severe weakness, numbness , loss of coordination, feeling unsteady. Common spironolactone side effects may include: breast swelling or tenderness.'},\n",
       " {'question': 'What are the side effects of minocycline?',\n",
       "  'source': 'https://www.drugs.com/minocycline.html',\n",
       "  'text': 'skin rash, fever, swollen glands, flu-like symptoms, muscle aches, severe weakness, unusual bruising, or yellowing of your skin or eyes. This may be more likely with long-term use of minocycline, and the reaction may occur several weeks after you began using this medicine. Call your doctor at once if you have: little or no urination, swelling in your feet or ankles, feeling tired or short of breath (signs of kidney problems); loss of appetite, upper stomach pain (that may spread to your back), nausea or vomiting, loss of appetite, easy bruising or bleeding, dark urine, yellowing of the skin or eyes (signs of liver or pancreas problems); joint pain or swelling with fever, swollen glands, muscle aches, chest pain, vomiting, unusual thoughts or behavior, and patchy skin color; severe headaches, ringing in your ears, dizziness, vision problems, pain behind your eyes; or swollen glands, flu symptoms, easy bruising or bleeding, severe tingling or numbness, muscle weakness, chest pain, new or worsening cough with fever, trouble breathing. Common minocycline side effects may include: numbness , tingling, burning pain; hair loss ; discoloration of you skin or nails. dizziness, spinning sensation; muscle or joint pain; nausea, diarrhea, loss of appetite; swollen tongue, cough, trouble swallowing; rash, itching; or headache .'},\n",
       " {'question': 'What are the side effects of Accutane?',\n",
       "  'source': 'https://www.drugs.com/accutane.html',\n",
       "  'text': 'problems with your vision or hearing; muscle or joint pain, bone pain, back pain; increased thirst, increased urination; hallucinations, (see or hearing things that are not real); symptoms of depression - unusual mood changes, crying spells, feelings of low self-worth, loss of interest in things you once enjoyed, new sleep problems, thoughts about hurting yourself; signs of liver or pancreas problems - loss of appetite, upper stomach pain (that may spread to your back), nausea or vomiting, fast heart rate , dark urine, jaundice (yellowing of the skin or eyes); severe stomach problems - severe stomach or chest pain, pain when swallowing, heartburn , diarrhea , rectal bleeding , bloody or tarry stools; or increased pressure inside the skull - severe headaches, ringing in your ears, dizziness, nausea, vision problems, pain behind your eyes. Common Accutane side effects may include: dryness of your skin, lips, eyes, or nose (you may have nosebleeds ); vision problems; headache, back pain , joint pain, muscle problems; skin reactions; or cold symptoms such as stuffy nose, sneezing, sore throat.'},\n",
       " {'question': 'What are the side effects of clindamycin?',\n",
       "  'source': 'https://www.drugs.com/mtm/clindamycin-topical.html',\n",
       "  'text': 'hives ; difficult breathing; swelling of your face, lips, tongue, or throat. Clindamycin topical may cause serious side effects. Stop using clindamycin topical and call your doctor at once if you have: severe redness, itching, or dryness of treated skin areas; or severe stomach pain, diarrhea that is watery or bloody (even if it occurs months after your last dose). Common side effects of clindamycin topical may include: burning, itching, dryness, peeling or redness of treated skin; or oily skin.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a7e10-18fc-495e-ab4a-da2f3ca00a97",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval with hit rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e7e79-5a2a-4247-bc3a-d093d0416761",
   "metadata": {},
   "source": [
    "Given a query, we check if our retriever pulling in the correct context to answer that query. If the LLM does not have the right context to answer the question, it cannot provide the right answer.\n",
    "\n",
    "For each query in our evaluation dataset, we will measure the following:\n",
    "1. Is the correct source included in any of the retrived chunks?\n",
    "2. What is the score our retriever gives to the correct source?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88e0d5f-a0ed-47e9-afcf-0ac882201e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad0afdc-2f44-4ca7-b5c0-2c6499f4f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "retriever = get_retriever(similarity_top_k=5, embedding_model_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b5438-9930-4857-a9a5-165cbd15c79b",
   "metadata": {},
   "source": [
    "Now let's evaluate our retriever.  We do this by checking how often the metadata (drug_link) of the top 5 retrieved sources matches the expected sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4cd2ae-17ff-4e75-904f-a577499bc8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are the side effects of doxycycline?', 'source': 'https://www.drugs.com/doxycycline.html', 'text': '(hives, difficult breathing, swelling in your face or throat) or a severe skin reaction (fever, sore throat, burning in your eyes, skin pain, red or purple skin rash that spreads and causes blistering and peeling). Seek medical treatment if you have a serious drug reaction that can affect many parts of your body. Symptoms may include: skin rash, fever, swollen glands, flu-like symptoms, muscle aches, severe weakness, unusual bruising, or yellowing of your skin or eyes. This reaction may occur several weeks after you began using doxycycline. Doxycycline may cause serious side effects. Call your doctor at once if you have: severe stomach pain, diarrhea that is watery or bloody; throat irritation, trouble swallowing; chest pain, irregular heart rhythm, feeling short of breath; little or no urination; low white blood cell counts - fever, chills, swollen glands, body aches, weakness, pale skin, easy bruising or bleeding; severe headaches, ringing in your ears, dizziness, nausea, vision problems, pain behind your eyes; loss of appetite, upper stomach pain (that may spread to your back), tiredness, nausea or vomiting, fast heart rate, dark urine, jaundice (yellowing of the skin or eyes). Common side effects of doxycycline may include: nausea and vomiting; upset stomach; loss of appetite; mild diarrhea; skin rash or itching; darkened skin color; vaginal itching or discharge.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2966/2966 [17:30<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "print(data[0])\n",
    "for entry in tqdm(data):\n",
    "    query = entry[\"question\"]\n",
    "    expected_source = entry['source']\n",
    "    \n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    retrieved_sources = [node.metadata['drug_link'] for node in retrieved_nodes]\n",
    "    \n",
    "    # If our label does not include a section, then any sections on the page should be considered a hit.\n",
    "    if \"#\" not in expected_source:\n",
    "        retrieved_sources = [source.split(\"#\")[0] for source in retrieved_sources]\n",
    "    \n",
    "    if expected_source in retrieved_sources:\n",
    "        is_hit = True\n",
    "        score = retrieved_nodes[retrieved_sources.index(expected_source)].score\n",
    "    else:\n",
    "        is_hit = False\n",
    "        score = 0.0\n",
    "    \n",
    "    result = {\n",
    "        \"is_hit\": is_hit,\n",
    "        \"score\": score,\n",
    "        \"retrieved\": retrieved_sources,\n",
    "        \"expected\": expected_source,\n",
    "        \"query\": query,\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12ecc64-efae-4a67-86de-3f3fbb36820b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_hit': True,\n",
       "  'score': 0.762622118,\n",
       "  'retrieved': ['https://www.drugs.com/doxycycline.html',\n",
       "   'https://www.drugs.com/cdi/doans-pills.html',\n",
       "   'https://www.drugs.com/mtm/doxylamine.html',\n",
       "   'https://www.drugs.com/cons/dolono.html',\n",
       "   'https://www.drugs.com/doxazosin.html'],\n",
       "  'expected': 'https://www.drugs.com/doxycycline.html',\n",
       "  'query': 'What are the side effects of doxycycline?'},\n",
       " {'is_hit': True,\n",
       "  'score': 0.724890232,\n",
       "  'retrieved': ['https://www.drugs.com/spironolactone.html',\n",
       "   'https://www.drugs.com/mtm/hydrochlorothiazide-and-spironolactone.html',\n",
       "   'https://www.drugs.com/sprix.html',\n",
       "   'https://www.drugs.com/spravato.html',\n",
       "   'https://www.drugs.com/cdi/kapspargo-sprinkle.html'],\n",
       "  'expected': 'https://www.drugs.com/spironolactone.html',\n",
       "  'query': 'What are the side effects of spironolactone?'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7d03f6-76bf-414d-a072-2862bdc2c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125421443020904"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hits = sum(result[\"is_hit\"] for result in results)\n",
    "hit_percentage = total_hits / len(results)\n",
    "hit_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd72fc2-fd48-48ab-bc32-bb9c43380904",
   "metadata": {},
   "source": [
    "So this retrieval technique gives us a hit 81.25% of the time OR the expected document is in the list of retrieved documents 81.25% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069c754b-30d9-432a-98f0-1bcec9d0fe99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578564292256575"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score = sum(result[\"score\"] for result in results) / len(results)\n",
    "average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21d243-4970-4c4c-8799-b45742744ca1",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval Using LLamaIndex RetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd02c0-270c-44b2-ad1a-57da99f7c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee6d56-f914-49c4-9ba9-66d0b8d272b8",
   "metadata": {},
   "source": [
    "## Step 9: Evaluating performance before Step 7 (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852dd79-247f-47d3-8ffa-97f79781a68a",
   "metadata": {},
   "source": [
    "### Golden Responses Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193ec2a-3a68-412b-bbb9-997e57b27edf",
   "metadata": {},
   "source": [
    "To effectively evaluate our generated responses, we need \"ground truth\" responses. These ground truth responses can be generated by **feeding the correct context to a golden LLM**. Then, we can use an LLM to evaluate our generated responses compared to the ground truth responses.\n",
    "\n",
    "We used the PaLM model (text-bison-1) here since it's been shown to be well aligned with human preferences and (that's the best we can get access to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54c9ab-8cec-4553-9003-f2de57d03e08",
   "metadata": {},
   "source": [
    "### Generating Golden Responses for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed04e2f-ffcd-446d-9c16-4cb9c3f226ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_responses(entries, llm):\n",
    "    context_window = llm.metadata.context_window - 500\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, context_window=context_window)\n",
    "    rs = get_response_synthesizer(service_context=service_context)\n",
    "\n",
    "    responses = []\n",
    "    for entry in tqdm(entries):\n",
    "        query = entry[\"question\"]\n",
    "        source = entry[\"source\"]\n",
    "\n",
    "        context = entry[\"text\"]\n",
    "        nodes = [NodeWithScore(node=TextNode(text=context))]\n",
    "\n",
    "        response = rs.synthesize(query, nodes=nodes)\n",
    "        responses.append(response.response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5c2a6-a5ac-4b9c-ab45-61d444b97b9b",
   "metadata": {},
   "source": [
    "We can now generate our reference responses. Let's generate 10 reference responses and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a164356-1fca-4f61-a9a0-ec525144a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47fae0",
   "metadata": {},
   "source": [
    "### PLEASE PASTE YOUR OWN API KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e872d22b-3836-428e-82e2-f86809c1013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "palm_api_key = \"\"\n",
    "palm.configure(api_key=palm_api_key)\n",
    "from llama_index.llms.palm import PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5981d038-4b6c-4a97-8e05-ccf9121ade7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:35<00:00, 15.55s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = PaLM(api_key=palm_api_key)\n",
    "ten_samples = data[:10]\n",
    "golden_responses = generate_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fc8506c-0deb-43ae-9936-cd323635cc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, golden_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23632aab-2be3-49cb-905b-1b26d7bde46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../datasets/golden-responses.json\", \"w\") as file:\n",
    "    json.dump(reference_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f07bb-d4b4-4e54-821a-b9bcf61804b1",
   "metadata": {},
   "source": [
    "### Generating Barebones Responses for gpt-3.5.turbo (no RAG) for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0ea07-3969-46cf-bd1b-b7d3f1796e1b",
   "metadata": {},
   "source": [
    "We will try to evaluate how the **gpt-3.5-turbo** model performs when given queries about the drug side effects but **no context**. This performance will be used to establish a baseline for comparing our RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d22cca8-58a6-403e-9d42-172acffda5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "def generate_bare_responses(entries, llm):\n",
    "    responses = []\n",
    "    for entry in tqdm(entries):\n",
    "        query = entry[\"question\"]\n",
    "        response = llm.complete(query)\n",
    "        responses.append(response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b35efb89-9aea-4efa-b5ab-768092798dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:47<00:00, 15.77s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-3.5-turbo', temperature=0.0, max_tokens=64) # max_tokens=512\n",
    "ten_samples = data[0:3]\n",
    "bare_responses1 = generate_bare_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c486c49-af6e-4ec6-8cb2-7c8a274bf22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:46<00:00, 15.60s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-3.5-turbo', temperature=0.0, max_tokens=64) # max_tokens=512\n",
    "ten_samples = data[3:6]\n",
    "bare_responses2 = generate_bare_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "790c41fb-e1a3-4e90-bf9b-55a93a2f1bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:12<00:00, 18.24s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-3.5-turbo', temperature=0.0, max_tokens=64) # max_tokens=512\n",
    "ten_samples = data[6:10]\n",
    "bare_responses3 = generate_bare_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96db9380-acb7-4deb-b732-651ec0e3b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_responses = bare_responses1 + bare_responses2 + bare_responses3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9e4c79b-dfac-4302-a11c-e1e0599184e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bare_responses)):\n",
    "    bare_responses[i] = str(bare_responses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f9748c3-2ca2-4785-a7f3-f5783a8a264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_samples = data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "351f2294-2fcd-45fa-9abc-e4d32bb0045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "barebones_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, bare_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "311bfe86-c396-459e-84f6-33777944a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(barebones_dataset))\n",
    "with open(\"../datasets/bare-responses-gpt.json\", \"w\") as file:\n",
    "    json.dump(barebones_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbf6c6-74ea-4802-be16-f96f2982a642",
   "metadata": {},
   "source": [
    "### Evaluating our bare LLM (gpt-3.5-turbo) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b00eaa-3357-46f4-8ad9-6b6cf3ec5ed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on Golden Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77338d73-69f9-4185-9d77-deef6b7ce49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69287020-d06b-4a24-af9c-35035a2ff612",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/bare-responses-gpt.json\", \"r\") as file:\n",
    "    bare_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a208b672-196b-47b5-9f2c-a21744d3b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import CorrectnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a462df-dc85-4ee9-83d7-b4fc082dba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "eval_llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=eval_llm)\n",
    "evaluator = CorrectnessEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd21d0d-eccd-4d9a-abba-8fcb519a49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [22:20<00:00, 134.06s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for bare_response, golden_response in tqdm(list(zip(bare_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = bare_response[\"response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe74c3-0ac1-46a5-aa8a-f05065427cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d20ab6-1e90-4321-bd01-bddb5d661e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e482a78-9d4e-4b07-95be-2f09ad7919d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval-scores-bare-gpt.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109d0f31-1941-4083-90f8-a488c1d325c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9175cc6",
   "metadata": {},
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/bare-responses-gpt.json\", \"r\") as file:\n",
    "    pred_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c914e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:29<00:00, 14.93s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for bare_response, golden_response in tqdm(list(zip(pred_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = bare_response[\"response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27a5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 3.0, 3.5, 3.0, 3.0, 3.5, 3.0, 4.0, 3.5, 3.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e6146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]\n",
    "with open(\"gpt3.5vshuman.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)\n",
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a30b13-2a59-4beb-8f85-27674c3950ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Industry Metrics on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce568e93-2adb-4d79-b182-9a04028c1725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/human1_responses.json\", \"r\") as file:\n",
    "    human1 = json.load(file)\n",
    "with open(\"../datasets/human2_responses.json\", \"r\") as file:\n",
    "    human2 = json.load(file)\n",
    "with open(\"../datasets/human3_responses.json\", \"r\") as file:\n",
    "    human3 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232bfd55-58da-4651-ae2b-75b99879edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/eval-scores-rag-gpt.json\", \"r\") as file:\n",
    "    rag = json.load(file)\n",
    "with open(\"../datasets/bare-responses-gpt.json\", \"r\") as file:\n",
    "    bare_llm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ae7121-bdf3-47c7-bbeb-e89696969d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "human1_responses = []\n",
    "human2_responses = []\n",
    "human3_responses = []\n",
    "rag_responses = []\n",
    "bare_responses = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    human1_responses.append(human1[i][\"response\"])\n",
    "    human2_responses.append(human2[i][\"response\"])\n",
    "    human3_responses.append(human3[i][\"response\"])\n",
    "    rag_responses.append(rag[i][\"generated_response\"])\n",
    "    bare_responses.append(bare_llm[i][\"response\"])\n",
    "references_dict = {\n",
    "    \"human_1\": human1_responses,\n",
    "    \"human_2\": human2_responses,\n",
    "    \"human_3\": human3_responses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c80ad1d-e2be-48fc-bbdf-9427e925c87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 16:01:35.543195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-06 16:01:35.543286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-06 16:01:35.544631: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 16:01:35.550869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 16:01:36.370345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Score...\n",
      "Calculating BLEU Score...\n",
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating METEOR Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from eval import generate_human_eval_summary\n",
    "barellm_vs_humans_result = generate_human_eval_summary(references_dict, bare_responses, \"Bare LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676aa7d5-54fa-4451-9cf6-2e6fa4150a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare LLM    rouge1    rouge2    rougeL  rougeLsum\n",
      "0  human_1  0.348891  0.133115  0.240907   0.240724\n",
      "1  human_2  0.329711  0.109742  0.225374   0.224765\n",
      "2  human_3  0.291540  0.105376  0.220748   0.220880\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fc3cae-af8d-4e91-9bd9-d5eb8e92cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare LLM     bleu\n",
      "0  human_1  0.03488\n",
      "1  human_2  0.03488\n",
      "2  human_3  0.03488\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9226418-436f-4d7e-9d75-c453834e384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare LLM    meteor\n",
      "0  human_1  0.172977\n",
      "1  human_2  0.172977\n",
      "2  human_3  0.172977\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"meteor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21578c9b-1897-412d-8052-1bedb17713d4",
   "metadata": {},
   "source": [
    "## Step 10: Evaluating our RAG Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f7ffb-8aa1-4e25-a666-aa82c0db4cb4",
   "metadata": {},
   "source": [
    "#### Using LlamaIndex Correctness evaluator on Golden Responses Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd15459b-a679-4f1b-832c-bb38cd467d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a53891-cd44-4efb-b9f9-9e3cc7de3a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_query_engine\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adebada-bce2-4600-ac2f-4ba541ce32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:27<00:00, 14.78s/it]\n"
     ]
    }
   ],
   "source": [
    "query_engine = get_query_engine(similarity_top_k=5, llm_model_name='gpt-3.5-turbo', embedding_model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Store both the original response object and the response string.\n",
    "rag_responses = []\n",
    "rag_response_str = []\n",
    "\n",
    "for entry in tqdm(golden_responses):\n",
    "    query = entry[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    rag_responses.append(response)\n",
    "    rag_response_str.append(response.response)\n",
    "store_rag_responses = rag_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17e76f6-7333-4ea7-9357-3bc125c59cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The side effects of doxycycline may include nausea and vomiting, upset stomach, loss of appetite, mild diarrhea, skin rash or itching, darkened skin color, vaginal itching or discharge.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_response_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a8456b-8a78-4e87-8110-3fd163f10864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import CorrectnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef83fc36-e93d-4341-b7a6-d0a24a598872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "eval_llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=eval_llm)\n",
    "evaluator = CorrectnessEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bb5e09-08e1-40e8-9129-0f441bbce72f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.48s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "for rag_response, golden_response in tqdm(list(zip(rag_response_str, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    generated_answer = rag_response\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=generated_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e27497-0ec6-4ae3-bc0a-1c342afb3047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5, 3.5, 3.5, 4.5, 4.0, 3.5, 4.5, 4.0, 5.0, 5.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190240d-7bb1-4079-8bfa-0d6d0ed9166d",
   "metadata": {},
   "source": [
    "Let's save the query, both responses, and the score to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc98a6d-a27a-4a42-a5c5-4574a336e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00e6239-5c44-4c1a-89d5-7549fb0ad98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval-scores-rag-gpt.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ef29b-b968-4449-9a50-4234a5b294ab",
   "metadata": {},
   "source": [
    "We can also calculate the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6b7d3a9-d2c5-4381-b2d4-9c1aa1d21df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80637d87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/eval-scores-rag-gpt.json\", \"r\") as file:\n",
    "    pred_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cdc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:25<00:00, 14.52s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for pred_response, golden_response in tqdm(list(zip(pred_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = pred_response[\"generated_response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5, 4.0, 3.5, 3.0, 3.5, 3.5, 4.5, 3.5, 4.5, 4.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c4ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]\n",
    "with open(\"gpt3.5ragvshuman.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)\n",
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba73bd-6f53-4570-8f42-42c204ef37a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Industry Metrics on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99631590-5e02-4b45-b68d-fbccf53783a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Score...\n",
      "Calculating BLEU Score...\n",
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating METEOR Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from eval import generate_human_eval_summary\n",
    "rag_vs_humans_result = generate_human_eval_summary(references_dict, rag_responses, \"RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ea7085-782f-4a9c-83d0-8254d9fd5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_vs_humans_result[\"meteor\"].to_csv(\"r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde2c457-d61d-445f-a1e1-368ee2a7ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RAG    rouge1    rouge2    rougeL  rougeLsum\n",
      "0  human_1  0.584965  0.441892  0.402719   0.401882\n",
      "1  human_2  0.567369  0.403966  0.379018   0.379128\n",
      "2  human_3  0.589005  0.489582  0.570795   0.572329\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f75648-762b-4437-91d5-c7846bd5da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RAG      bleu\n",
      "0  human_1  0.377064\n",
      "1  human_2  0.377064\n",
      "2  human_3  0.377064\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5059cfa3-cfbe-4945-9426-c3406441fb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RAG    meteor\n",
      "0  human_1  0.474049\n",
      "1  human_2  0.474049\n",
      "2  human_3  0.474049\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"meteor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cd34a-b2f4-41ab-8fbd-fabe238f5321",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluation Summary of Bare LLM vs RAG in Industry Standard Metrics on Golden Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85338c58-4307-4318-a04f-b627e71f23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/bare-responses-gpt.json\", \"r\") as file:\n",
    "    bare_llm = json.load(file)\n",
    "with open(\"../datasets/eval-scores-rag-gpt.json\", \"r\") as file:\n",
    "    rag = json.load(file)\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    golden = json.load(file)\n",
    "\n",
    "rag_responses = []\n",
    "bare_responses = []\n",
    "golden_responses = []\n",
    "for i in range(0, 10):\n",
    "    rag_responses.append(rag[i][\"generated_response\"])\n",
    "    bare_responses.append(bare_llm[i][\"response\"])\n",
    "    golden_responses.append(golden[i][\"response\"])\n",
    "    \n",
    "predictions_dict = {\n",
    "    \"Bare LLM\": bare_responses,\n",
    "    \"RAG\": rag_responses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150c29f-34e1-49d6-b535-a81d0e9fd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import generate_metrics_summary\n",
    "result = generate_metrics_summary(golden_responses, predictions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d36bee-ef72-4b48-8191-c5c236d6b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     System    rouge1    rouge2    rougeL  rougeLsum\n",
      "0  Bare LLM  0.410927  0.172764  0.338584   0.335817\n",
      "1       RAG  0.729363  0.690272  0.732632   0.728783\n"
     ]
    }
   ],
   "source": [
    "print(result[\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ccdf93-9c9e-4dcd-a0fc-71fb22c06426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     System      bleu\n",
      "0  Bare LLM  0.074376\n",
      "1       RAG  0.594337\n"
     ]
    }
   ],
   "source": [
    "print(result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1aa0c05-c1e3-4c94-8f10-dceb40b9e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     System  average_bertscore_precision  average_bertscore_recall  \\\n",
      "0  Bare LLM                     0.867267                  0.879961   \n",
      "1       RAG                     0.949020                  0.970628   \n",
      "\n",
      "   average_bertscore_f1  \n",
      "0              0.872771  \n",
      "1              0.958935  \n"
     ]
    }
   ],
   "source": [
    "print(result[\"bert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39839fd-bbcf-4ae7-a094-da6fe60a150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     System    meteor\n",
      "0  Bare LLM  0.339511\n",
      "1       RAG  0.801181\n"
     ]
    }
   ],
   "source": [
    "print(result[\"meteor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0229cb-2256-4762-a772-55740e617951",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluation without Golden Responses and User Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b1970-ded0-4ee4-b49e-49447fad1533",
   "metadata": {},
   "source": [
    "Generating reference responses and then using them for evaluation can give us a probably very accurate assesment on how our query engine is performing. However, this approach can be expensive and biased to the reference dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905688d-9a0d-4e2a-81ea-40242f50905a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluating for faithfulness/relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c767913f-fc0a-4437-932a-4a5106a5768c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from llama_index import ServiceContext\n",
    "# from llama_index.llms import OpenAI\n",
    "import google.generativeai as palm\n",
    "\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "palm.configure(api_key=palm_api_key)\n",
    "from llama_index.llms.palm import PaLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "def evaluate(queries: list, responses: list, metric: str):\n",
    "    llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    \n",
    "    if metric == 'faithfulness':\n",
    "        evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "    elif metric == 'relevancy':\n",
    "        evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown metric: \", metric)\n",
    "\n",
    "    evals = []\n",
    "    for query, response in tqdm(list(zip(queries, responses))):\n",
    "        eval_result = evaluator.evaluate_response(query=query, response=response)\n",
    "        evals.append(eval_result)\n",
    "    \n",
    "    return evals\n",
    "\n",
    "def get_pass_rate(evals):\n",
    "    return len([val.passing for val in evals]) / len(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd11db-ced6-433b-844d-03c6d0adc30a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "faithfulness_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses, metric='faithfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed4555fa-5901-49df-8f66-afbe00d27079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_score = get_pass_rate(faithfulness_results)\n",
    "faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e8a04f-62f6-46cb-80e1-adde0195c1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9/9 [02:14<00:00, 14.99s/it]\n"
     ]
    }
   ],
   "source": [
    "relevancy_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=store_rag_responses, metric='relevancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61b35f60-3989-459c-a26e-12ed35456e8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevancy_score = get_pass_rate(relevancy_results)\n",
    "relevancy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e9f36-5476-4e88-a4ee-74c360e81e8f",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe884d-c488-4bf9-bb39-ded0bc4be1e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40223c27",
   "metadata": {},
   "source": [
    "### PLEASE PASTE YOUR OWN API KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e4496d-0508-4e09-a3f9-47a012522b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb327758-ef20-4bc3-ac24-f9fdba7e7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditi/anaconda3/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "api_key = \"\"\n",
    "pinecone.init(api_key=api_key, environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26955c0-ec42-4452-ac3d-6e3a70a34375",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(\"langchain-retrieval-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c797918e-fd94-48b5-a875-fce5da91fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding\n",
    "\n",
    "def get_embedding_model(model_name, embed_batch_size=100):\n",
    "    if model_name == \"text-embedding-ada-002\":\n",
    "            return OpenAIEmbedding(\n",
    "                model=model_name,\n",
    "                embed_batch_size=embed_batch_size,\n",
    "                api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    else:\n",
    "        return HuggingFaceEmbedding(\n",
    "            model_name=model_name,\n",
    "            embed_batch_size=embed_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00e44-ab26-4f5e-8162-23961dacd067",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 9: Evaluating performance before Step 7 (bare Mistral w/o RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e22388b-c656-433b-8672-bce384c5f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "def generate_bare_responses(entries, llm):\n",
    "    responses = []\n",
    "    for entry in tqdm(entries):\n",
    "        query = entry[\"question\"]\n",
    "        response = llm.complete(query)\n",
    "        responses.append(response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aa3756-faf0-4a98-8439-2743f67dac29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adca3b7e-62a5-420f-bf19-67642a686174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:51<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Replicate\n",
    "llm = Replicate(\n",
    "    model=\"mistralai/mistral-7b-v0.1:3e8a0fb6d7812ce30701ba597e5080689bef8a013e5c6a724fafb108cc2426a0\"\n",
    ")\n",
    "ten_samples = data[0:10]\n",
    "bare_responses = generate_bare_responses(ten_samples, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ceab4a-15df-4e41-82e7-29d93560df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_responses = [str(i) for i in bare_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a69238c-5290-4bf0-b5da-7d91414b01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "barebones_dataset = [{\"question\": entry[\"question\"], \"source\": entry[\"source\"], \"response\": response} for entry, response in zip(ten_samples, bare_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdae85d6-0d64-484c-89ce-f5081903c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(barebones_dataset))\n",
    "with open(\"../datasets/bare-responses-mistral.json\", \"w\") as file:\n",
    "    json.dump(barebones_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4c919-5116-4365-a1a1-60a19b8ed4ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on Golden Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a80b85f9-f374-4fa6-913b-32dbac464123",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/bare-responses-mistral.json\", \"r\") as file:\n",
    "    bare_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19b47afb-9e60-48db-8a92-fbf0436e3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "eval_llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=eval_llm)\n",
    "evaluator = CorrectnessEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99df9cb1-f5ec-42bf-9193-e261ba457d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a780a66-a868-459a-9e47-5732aed6cc16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▏                                                          | 3/10 [00:18<00:42,  6.06s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [00:28<00:46,  7.83s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [00:46<00:56, 11.23s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [01:11<01:03, 15.91s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [01:28<00:49, 16.43s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [01:47<00:34, 17.27s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [02:03<00:16, 16.71s/it]WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-LI2Pi1I4jc5dAS6dtdz1UTMo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:32<00:00, 15.20s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for bare_response, golden_response in tqdm(list(zip(bare_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = bare_response[\"response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e810aba-7f6a-4798-8794-6b96ba4ff8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5, 3.5, 3.5, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e61bd9d5-b89b-4d99-bbbc-d27ab948ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "097eb023-7703-4f86-8752-1cc7b268fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval-scores-bare-mistral.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e816a88b-e4ca-4765-b3f2-4951379917e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beae406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/eval-scores-bare-mistral.json\", \"r\") as file:\n",
    "    pred_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43658f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:29<00:00, 14.94s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for pred_response, golden_response in tqdm(list(zip(pred_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = pred_response[\"generated_response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 3.5, 3.5, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 3.5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2661571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]\n",
    "with open(\"mistralvshuman.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)\n",
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd63f28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Industry Metrics on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/human1_responses.json\", \"r\") as file:\n",
    "    human1 = json.load(file)\n",
    "with open(\"../datasets/human2_responses.json\", \"r\") as file:\n",
    "    human2 = json.load(file)\n",
    "with open(\"../datasets/human3_responses.json\", \"r\") as file:\n",
    "    human3 = json.load(file)\n",
    "with open(\"../datasets/eval-scores-rag-mistral.json\", \"r\") as file:\n",
    "    rag = json.load(file)\n",
    "with open(\"../datasets/bare-responses-mistral.json\", \"r\") as file:\n",
    "    bare_llm = json.load(file)\n",
    "with open(\"../datasets/human1_responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1417fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "human1_responses = []\n",
    "human2_responses = []\n",
    "human3_responses = []\n",
    "rag_responses = []\n",
    "bare_responses = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    human1_responses.append(human1[i][\"response\"])\n",
    "    human2_responses.append(human2[i][\"response\"])\n",
    "    human3_responses.append(human3[i][\"response\"])\n",
    "    rag_responses.append(rag[i][\"generated_response\"])\n",
    "    bare_responses.append(bare_llm[i][\"response\"])\n",
    "references_dict = {\n",
    "    \"human_1\": human1_responses,\n",
    "    \"human_2\": human2_responses,\n",
    "    \"human_3\": human3_responses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Score...\n",
      "Calculating BLEU Score...\n",
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating METEOR Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from eval import generate_human_eval_summary\n",
    "barellm_vs_humans_result = generate_human_eval_summary(references_dict, bare_responses, \"Bare Mistral-LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare Mistral-LLM    rouge1    rouge2    rougeL  rougeLsum\n",
      "0          human_1  0.282957  0.084747  0.167597   0.184145\n",
      "1          human_2  0.282700  0.075186  0.167647   0.187259\n",
      "2          human_3  0.266228  0.069078  0.170472   0.198220\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare Mistral-LLM      bleu\n",
      "0          human_1  0.040892\n",
      "1          human_2  0.040892\n",
      "2          human_3  0.040892\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare Mistral-LLM    meteor\n",
      "0          human_1  0.195242\n",
      "1          human_2  0.195242\n",
      "2          human_3  0.195242\n"
     ]
    }
   ],
   "source": [
    "print(barellm_vs_humans_result[\"meteor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4c304-36f6-4619-9435-076220c44300",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 10: Evaluating our RAG Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25c1638-abd4-4e66-ab24-a83305313d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Replicate\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "\n",
    "mistral = Replicate(\n",
    "    model=\"mistralai/mistral-7b-v0.1:3e8a0fb6d7812ce30701ba597e5080689bef8a013e5c6a724fafb108cc2426a0\"\n",
    ")\n",
    "embedding_model = get_embedding_model(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=mistral)\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index,\n",
    "    add_sparse_vector=True,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533363d6-5b2c-42aa-811c-df3360a00d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:52<00:00,  5.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store both the original response object and the response string.\n",
    "rag_responses = []\n",
    "rag_response_str = []\n",
    "from tqdm import tqdm\n",
    "for entry in tqdm(golden_responses):\n",
    "    query = entry[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    rag_responses.append(response)\n",
    "    rag_response_str.append(response.response)\n",
    "store_mistral_rag_responses = rag_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087757c-dad3-4f76-9b22-47dc0452e626",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on Golden Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c110a02-5e1b-4d14-9a10-465d31a8679f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "with open(\"../datasets/human1_responses.json\", \"r\") as file:\n",
    "    golden_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c0d0d2-4763-4948-a97b-67401361a6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "eval_llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=eval_llm)\n",
    "evaluator = CorrectnessEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b72291-f01f-4f18-8f19-8a60e8569d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response_str = ['The side effects of doxycycline can include nausea and vomiting, upset stomach, loss of appetite, mild diarrhea, skin rash or itching, darkened skin color, vaginal itching or discharge, severe stomach pain, diarrhea that is watery or bloody, throat irritation, trouble swallowing, chest pain, irregular heart rhythm, feeling short of breath, little or no urination, low white blood cell counts - fever, chills, swollen glands, body aches, weakness, pale skin, easy bruising or bleeding, severe headaches, ring', 'Based on the context information provided, some common side effects of spironolactone may include breast swelling or tenderness. However, it is important to note that spironolactone may also cause more serious side effects, such as high potassium levels, low potassium levels, and low sodium levels, which can lead to symptoms such as nausea, weakness, chest pain, irregular heartbeats, and loss of movement. It is important to consult with a healthcare professional for a complete list of potential side effects and to discuss any concerns you may have.', 'The side effects of minocycline can include:\\n\\n* Nausea, vomiting, loss of appetite, diarrhea, and abdominal pain\\n* Headache, dizziness, and lightheadedness\\n* Hair loss\\n* Skin rash, itching, and discoloration\\n* Joint pain, muscle aches, and weakness\\n* Fatigue, weakness, and weight loss\\n* Flu-like symptoms, such as fever, chills, and sore throat\\n* Difficulty swallowing, swelling of the tongue, and mouth sores\\n* Chest', 'The side effects of Accutane can include dryness of the skin, lips, eyes, or nose (with possible nosebleeds), vision problems, headache, back pain, joint pain, muscle problems, skin reactions, and cold symptoms such as stuffy nose, sneezing, and sore throat. Severe side effects can include severe stomach or chest pain, pain when swallowing, heartburn, diarrhea, rectal bleeding, bloody or tarry stools, increased pressure inside the skull (with severe headaches, ringing in the ears, dizziness, nausea, vision problems, and', 'The side effects of clindamycin can include burning, itching, dryness, peeling or redness of treated skin; oily skin, nausea, vomiting, stomach pain, mild skin rash, or vaginal itching or discharge. Severe side effects can include severe redness, itching, or dryness of treated skin areas; severe stomach pain, diarrhea that is watery or bloody (even if it occurs months after your last dose); or a severe skin reaction (fever, sore throat, burning in your eyes, skin pain, red or purple skin rash that', 'The side effects of Aldactone may include breast swelling or tenderness.', 'The side effects of tretinoin may include severe burning, stinging, or irritation of treated skin; severe skin dryness; or severe redness, swelling, blistering, peeling, or crusting. Your skin may be more sensitive to weather extremes such as cold and wind while using tretinoin topical. Common side effects of tretinoin topical may include skin pain, redness, burning, itching, or irritation; sore throat ; mild warmth or stinging where the medicine was applied; or changes in color of treated skin.', 'The side effects of isotretinoin include: dryness of skin, lips, eyes, or nose; vision problems; headache, back pain, joint pain, muscle problems; skin reactions; or cold symptoms such as stuffy nose, sneezing, sore throat.\\n\\nIt is important to note that not all patients will experience these side effects, and some may be more severe or last longer than others. If you experience any side effects, it is important to speak with your healthcare provider, as they may be able to adjust your treatment plan or recommend additional medications to help manage your symptoms.', 'The side effects of Bactrim may include nausea, vomiting, loss of appetite; or skin rash. More severe side effects may include severe stomach pain, diarrhea that is watery or bloody, yellowing of your skin or eyes, seizure, new or unusual joint pain, increased or decreased urination, swelling, bruising, or irritation around the IV needle, increased thirst, dry mouth, fruity breath odor, new or worsening cough, fever, trouble breathing, high blood potassium, low blood sodium, or low blood cell counts. If you', 'The side effects of Retin-A may include mild warmth or stinging where the medicine was applied; or changes in color of treated skin. Severe side effects may include severe burning, stinging, or irritation of treated skin; severe redness, swelling, blistering, peeling, or crusting; or difficulty breathing, swelling of the face, lips, tongue, or throat.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90444411-9420-41db-9176-efc361e9974e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:30<00:00, 15.03s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for rag_response, golden_response in tqdm(list(zip(rag_response_str, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    generated_answer = rag_response\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=generated_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4162b3c-f03c-4dd0-b2f4-11793bf9b44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5, 3.5, 3.0, 3.5, 3.5, 5.0, 4.5, 4.5, 3.5, 3.5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863456e-3da3-4c76-95c0-69a989ceba6b",
   "metadata": {},
   "source": [
    "Let's save the query, both responses, and the score to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4e5229-246f-4383-91f5-ec5ce9f75971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6749a7ed-2fc9-4309-901a-6bc3f4301bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"eval-scores-rag-mistral.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4fff9-3446-4a34-b5c5-b603a81c7c84",
   "metadata": {},
   "source": [
    "We can also calculate the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9447019b-71d6-434c-b1bc-f93c4bf61133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659477b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using the LLamaIndex Correctness Evaluator on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5076be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../datasets/eval-scores-rag-mistral.json\", \"r\") as file:\n",
    "    pred_responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eefdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.44s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "from tqdm import tqdm\n",
    "for pred_response, golden_response in tqdm(list(zip(pred_responses, golden_responses))):\n",
    "    query = golden_response[\"question\"]\n",
    "    golden_answer = golden_response[\"response\"]\n",
    "    bare_answer = pred_response[\"generated_response\"]\n",
    "    \n",
    "    eval_result = evaluator.evaluate(query=query, reference=golden_answer, response=bare_answer)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214b0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 3.5, 3.5, 4.5, 3.5, 3.5, 4.5, 4.5, 4.5, 3.5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[r.score for r in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07decc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [\n",
    "    {\"question\": golden_response[\"question\"],\n",
    "     \"golden_response\": golden_response[\"response\"],\n",
    "     \"generated_response\": eval_result.response,\n",
    "     \"score\": eval_result.score,\n",
    "     \"reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    for eval_result, golden_response in zip(eval_results, golden_responses)\n",
    "]\n",
    "with open(\"mistralragvshuman.json\", \"w\") as file:\n",
    "    json.dump(scores, file, indent=4)\n",
    "average_scores = sum(score[\"score\"] for score in scores) / len(scores)\n",
    "average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7137470",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Industry metrics on User Responses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cf1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Score...\n",
      "Calculating BLEU Score...\n",
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating METEOR Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from eval import generate_human_eval_summary\n",
    "rag_vs_humans_result = generate_human_eval_summary(references_dict, rag_responses, \"Mistral+RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa291ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mistral+RAG    rouge1    rouge2    rougeL  rougeLsum\n",
      "0     human_1  0.599769  0.461251  0.521128   0.529843\n",
      "1     human_2  0.587186  0.427751  0.499804   0.506154\n",
      "2     human_3  0.490641  0.335950  0.352346   0.367209\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d06ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mistral+RAG      bleu\n",
      "0     human_1  0.307101\n",
      "1     human_2  0.307101\n",
      "2     human_3  0.307101\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mistral+RAG    meteor\n",
      "0     human_1  0.416993\n",
      "1     human_2  0.416993\n",
      "2     human_3  0.416993\n"
     ]
    }
   ],
   "source": [
    "print(rag_vs_humans_result[\"meteor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737471ce-511f-46ee-8b86-36b5baca5674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Industry standard metric summary of gpt-3.5-turbo RAG vs Mistral RAG on golden context dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7596be-3920-472f-b625-aa95f78ec6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Score...\n",
      "Calculating BLEU Score...\n",
      "Calculating BERT Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating METEOR Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../datasets/bare-responses-gpt.json\", \"r\") as file:\n",
    "    bare_llm = json.load(file)\n",
    "with open(\"../datasets/bare-responses-mistral.json\", \"r\") as file:\n",
    "    bare_llm_mistral = json.load(file)\n",
    "with open(\"../datasets/eval-scores-rag-gpt.json\", \"r\") as file:\n",
    "    gpt_rag = json.load(file)\n",
    "with open(\"../datasets/golden-responses.json\", \"r\") as file:\n",
    "    golden = json.load(file)\n",
    "with open(\"../datasets/eval-scores-rag-mistral.json\", \"r\") as file:\n",
    "    mistral_rag = json.load(file)\n",
    "\n",
    "gpt_rag_responses = []\n",
    "bare_responses = []\n",
    "bare_mistral_responses = []\n",
    "golden_responses = []\n",
    "mistral_rag_responses = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    gpt_rag_responses.append(gpt_rag[i][\"generated_response\"])\n",
    "    mistral_rag_responses.append(mistral_rag[i][\"generated_response\"])\n",
    "    bare_responses.append(bare_llm[i][\"response\"])\n",
    "    bare_mistral_responses.append(bare_llm_mistral[i][\"response\"])\n",
    "    golden_responses.append(golden[i][\"response\"])\n",
    "    \n",
    "predictions_dict = {\n",
    "    \"Bare gpt-3.5.-turbo\": bare_responses,\n",
    "    \"Bare mistral-7b-instruct-v0.1\": bare_mistral_responses,\n",
    "    \"gpt-3.5-RAG\": gpt_rag_responses,\n",
    "    \"mistral-RAG\": mistral_rag_responses\n",
    "}\n",
    "from eval import generate_metrics_summary\n",
    "result = generate_metrics_summary(golden_responses, predictions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c374b4ad-5687-4c38-b130-97c718462e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          System    rouge1    rouge2    rougeL  rougeLsum\n",
      "0            Bare gpt-3.5.-turbo  0.410056  0.169581  0.334222   0.337933\n",
      "1  Bare mistral-7b-instruct-v0.1  0.252921  0.069907  0.173862   0.196391\n",
      "2                    gpt-3.5-RAG  0.729535  0.692276  0.725238   0.724041\n",
      "3                    mistral-RAG  0.619861  0.501489  0.479023   0.494634\n"
     ]
    }
   ],
   "source": [
    "print(result['rouge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09bb9d09-1b17-42db-83df-33f9a213b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          System      bleu\n",
      "0            Bare gpt-3.5.-turbo  0.074376\n",
      "1  Bare mistral-7b-instruct-v0.1  0.039006\n",
      "2                    gpt-3.5-RAG  0.594337\n",
      "3                    mistral-RAG  0.365213\n"
     ]
    }
   ],
   "source": [
    "print(result['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c35fdbd0-8c3a-41d3-9ddc-1f5f61b548b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          System    meteor\n",
      "0            Bare gpt-3.5.-turbo  0.339511\n",
      "1  Bare mistral-7b-instruct-v0.1  0.239231\n",
      "2                    gpt-3.5-RAG  0.801181\n",
      "3                    mistral-RAG  0.609384\n"
     ]
    }
   ],
   "source": [
    "print(result['meteor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b4b5a-9528-4cc0-a804-10ac87f7e2de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Faithfulness and Relevance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0436f20e-82a0-4b63-a0ef-01c2ea7badb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from llama_index import ServiceContext\n",
    "# from llama_index.llms import OpenAI\n",
    "import google.generativeai as palm\n",
    "\n",
    "palm_api_key = \"AIzaSyBCDSREHajiFWH65cWEl4BlXfuAG7HjRS0\"\n",
    "palm.configure(api_key=palm_api_key)\n",
    "from llama_index.llms.palm import PaLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "def evaluate(queries: list, responses: list, metric: str):\n",
    "    llm = PaLM(api_key=palm_api_key, temperature=0.0)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    \n",
    "    if metric == 'faithfulness':\n",
    "        evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "    elif metric == 'relevancy':\n",
    "        evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown metric: \", metric)\n",
    "\n",
    "    evals = []\n",
    "    for query, response in tqdm(list(zip(queries, responses))):\n",
    "        eval_result = evaluator.evaluate_response(query=query, response=response)\n",
    "        evals.append(eval_result)\n",
    "    \n",
    "    return evals\n",
    "\n",
    "def get_pass_rate(evals):\n",
    "    return len([val.passing for val in evals]) / len(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f67ee-411b-4f1a-abdc-0915d9940768",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "faithfulness_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=rag_responses, metric='faithfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "520775e5-c928-4a00-ab5f-8aeef2b6007f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_score = get_pass_rate(faithfulness_results)\n",
    "faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad239c0a-6e6a-43e5-ba19-ba4bbe474899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9/9 [02:14<00:00, 14.99s/it]\n"
     ]
    }
   ],
   "source": [
    "relevancy_results = evaluate(queries=[sample[\"question\"] for sample in ten_samples], responses=store_mistral_rag_responses, metric='relevancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45256ae2-eedf-4323-bfd9-7500af9ab5c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevancy_score = get_pass_rate(relevancy_results)\n",
    "relevancy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
